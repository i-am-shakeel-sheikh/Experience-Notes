optimal parameters 

outcomes of measurements follow probability distributions defined by the story of how the data came to be 

the parameters mean,std used in np.random.normal(mean,std,size=100000)

calculated directly from data 

parameters that bring the model in closest aggrement with the data 

rely on built in numpy functions to find the optimal params 
great tools in python ecosystem for doing statistical inference , including by optimization 

scipy.stats 
statsmodels 
hacker stats with numpy 

optional params to fit theorical model distribution and data 

# Seed random number generator
np.random.seed(42)

# Compute mean no-hitter time: tau
tau = np.mean(nohitter_times)

# Draw out of an exponential distribution with parameter tau: inter_nohitter_time
inter_nohitter_time = np.random.exponential(tau, 100000)

# Plot the PDF and label axes
_ = plt.hist(inter_nohitter_time,
             bins=50, normed=True, histtype='step')
_ = plt.xlabel('Games between no-hitters')
_ = plt.ylabel('PDF')

# Show the plot
plt.show()

games between no-hitters

-------------------

# Create an ECDF from real data: x, y
x, y = ecdf(nohitter_times)

# Create a CDF from theoretical samples: x_theor, y_theor
x_theor, y_theor = ecdf(inter_nohitter_time)

# Overlay the plots
plt.plot(x_theor, y_theor)
plt.plot(x, y, marker='.', linestyle='none')

# Margins and axis labels
plt.margins(0.02)
plt.xlabel('Games between no-hitters')
plt.ylabel('CDF')

# Show the plot
plt.show()

It looks like no-hitters in the modern era of Major League Baseball are Exponentially distributed. 
Based on the story of the Exponential distribution, this suggests that they are a random process; 
when a no-hitter will happen is independent of when the last no-hitter was.
-------------------

How is this parameter optimal


# Plot the theoretical CDFs
plt.plot(x_theor, y_theor)
plt.plot(x, y, marker='.', linestyle='none')
plt.margins(0.02)
plt.xlabel('Games between no-hitters')
plt.ylabel('CDF')

# Take samples with half tau: samples_half
samples_half = np.random.exponential(tau/2,10000)

# Take samples with double tau: samples_double
samples_double = np.random.exponential(2*tau,10000)

# Generate CDFs from these samples
x_half, y_half = ecdf(samples_half)
x_double, y_double = ecdf(samples_double)

# Plot these CDFs as lines
_ = plt.plot(x_half, y_half)
_ = plt.plot(x_double, y_double)

# Show the plot
plt.show()

Great work! Notice how the value of tau given by the mean matches the data best. In this way, tau is an optimal parameter.
----------------------

Linear Regression by least squares 

2008 us swing election results 

x -> total votes 
y -> percent of vote for obama 

pearson correlation coefficient is important to compute but we like to get fuller understanding of how data are related to each other 
we might suspect some underlying function gives the data its shape 
parameters of function is slope , intercept 

data points collectively lie as close as possible to the line 

vertical distance between data point and the line is residual 

we define the line that is closest to the data to be the line for which the sum of squares of residual is minimal 

The process of finding the parameters for which the sum of the squares of the residula is minimal is called least squares 


np.polyfit() 

performs least square analysis with polynomial functions 

linear function is a first degree polynomial 

slope,intercept = np.polyfit(total_votes,dem_share,1)

slope
4.03*e-05

tells that 4 more percent votes for every 100000 additional voters in a county
intercept

----------------

EDA of literacy/fertility data 

# Plot the illiteracy rate versus fertility
_ = plt.plot(illiteracy, fertility, marker='.', linestyle='none')

# Set the margins and label axes
plt.margins(0.002)
_ = plt.xlabel('percent illiterate')
_ = plt.ylabel('fertility')

# Show the plot
plt.show()

# Show the Pearson correlation coefficient
print(pearson_r(illiteracy, fertility))

You can see the correlation between illiteracy and fertility by eye, and by the substantial Pearson correlation coefficient of 0.8. 
It is difficult to resolve in the scatter plot, but there are many points around near-zero illiteracy and about 1.8 children/woman.


# Plot the illiteracy rate versus fertility
_ = plt.plot(illiteracy, fertility, marker='.', linestyle='none')
plt.margins(0.02)
_ = plt.xlabel('percent illiterate')
_ = plt.ylabel('fertility')

# Perform a linear regression using np.polyfit(): a, b
a, b = np.polyfit(illiteracy,fertility,1)

# Print the results to the screen
print('slope =', a, 'children per woman / percent illiterate')
print('intercept =', b, 'children per woman')

# Make theoretical line to plot
x = np.array([0,100])
y = a * x + b

# Add regression line to your plot
_ = plt.plot(x, y)

# Draw the plot
plt.show()


----------------------

np.polyfit() optimizes the sum of squares of residuals also known as RSS 

RSS vs slope 

https://www.pluralsight.com/guides/different-ways-create-numpy-arrays

# Specify slopes to consider: a_vals
a_vals = np.linspace(0,0.1,200)

# Initialize sum of square of residuals: rss
rss = np.empty_like(a_vals)

# Compute sum of square of residuals for each value of a_vals
for i, a in enumerate(a_vals):
    rss[i] = np.sum((fertility - a*illiteracy - b)**2)

# Plot the RSS
plt.plot(a_vals, rss, '-')
plt.xlabel('slope (children per woman / percent illiterate)')
plt.ylabel('sum of square of residuals')

plt.show()

parabola 

Notice that the minimum on the plot, that is the value of the slope that gives the minimum sum of the square of the residuals, is the same value you got when performing the regression.


https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data/more-on-regression/v/r-squared-or-coefficient-of-determination

------------------------


importance of EDA: Anscombes quartet 

published a papaer that contains 4 fictious x-y datasets plotted here 

if we have same x bar and y bar. 

when we try to plot regression , they all will have same line 

surely some of the fits are less optimal than others 

construct confidence intervals which quantify uncertainity about the parameter estimates 


# Perform linear regression: a, b
a, b = np.polyfit(x,y,1)

# Print the slope and intercept
print(a, b)

# Generate theoretical x and y data: x_theor, y_theor
x_theor = np.array([3, 15])
y_theor = a * x_theor + b

# Plot the Anscombe data and theoretical line
_ = plt.plot(x,y,marker='.',linestyle='none')
_ = plt.plot(x_theor,y_theor,marker='.',linestyle='none')

# Label the axes
plt.xlabel('x')
plt.ylabel('y')

# Show the plot
plt.show()


Anscombe pairs

# Iterate through x,y pairs
for x, y in zip(anscombe_x, anscombe_y):
    # Compute the slope and intercept: a, b
    a, b = np.polyfit(x,y,1)

    # Print the result
    print('slope:', a, 'intercept:', b)

--------------------

