how to assess that our observed data are actually described by the model 

Ohio, pensylvania ; urban counties and rural counties 

i hypothsise that county level voting in these two states have identical probability distributions 

Hypothesis testing 

null Hypothesis

x axis -> percent of vote for obama 
y axis -> ecdf


pensylvania seems to be slightly more towards obama in middle part of ecdf 

percent vote for obama 

mean
median 
std 

pensylvania: 67 counties 
ohio : 88 counties 

permutation : take first 67 

it is at the heart of simulating a null hypothsis we assume that two quantities are identically distributed


import numpy as np 
dem_share_both = np.concatenate((dem_share_PA,dem_share_OH))
dem_share_perm = np.random.permutation(dem_share_both)
perm_sample_PA = dem_share_perm[:len(dem_share_PA)]
perm_sample_OH = dem_share_perm[len(dem_share_PA):]


------------------

def permutation_sample(data1, data2):
    """Generate a permutation sample from two data sets."""

    # Concatenate the data sets: data
    data = np.concatenate((data1,data2))

    # Permute the concatenated array: permuted_data
    permuted_data = np.random.permutation(data)

    # Split the permuted array into two: perm_sample_1, perm_sample_2
    perm_sample_1 = permuted_data[:len(data1)]
    perm_sample_2 = permuted_data[len(data1):]

    return perm_sample_1, perm_sample_2

----------------

visualizing permutation sampling 

for i in range(50):
    # Generate permutation samples
    perm_sample_1, perm_sample_2 = permutation_sample(rain_june,rain_november)


    # Compute ECDFs
    x_1, y_1 = ecdf(perm_sample_1)
    x_2, y_2 = ecdf(perm_sample_2)

    # Plot ECDFs of permutation sample
    _ = plt.plot(x_1, y_1, marker='.', linestyle='none',
                 color='red', alpha=0.02)
    _ = plt.plot(x_2, y_2, marker='.', linestyle='none',
                 color='blue', alpha=0.02)

# Create and plot ECDFs from original data
x_1, y_1 = ecdf(rain_june)
x_2, y_2 = ecdf(rain_november)
_ = plt.plot(x_1, y_1, marker='.', linestyle='none', color='red')
_ = plt.plot(x_2, y_2, marker='.', linestyle='none', color='blue')

# Label axes, set margin, and show plot
plt.margins(0.02)
_ = plt.xlabel('monthly rainfall (mm)')
_ = plt.ylabel('ECDF')
plt.show()

Great work! Notice that the permutation samples ECDFs overlap and give a purple haze.
 None of the ECDFs from the permutation samples overlap with the observed data, suggesting that the hypothesis is not commensurate with the data. 
 July and November rainfall are not identically distributed.

---------------------------

Test statistics and p values 

Assessment of how reasonable the observed data are assuming hypothesis is true 

single number that can be computed from observed data and from data you simulate under the null hypothesis

basis of comparison of what the hypothesis predicts and what we actually observed

permuataion replicate 
np.mean(perm_sample_PA) - np.mean(perm_sample_OH)
np.mean(perm_sample_PA) - np.mean(perm_sample_OH)

Mean vote difference under null hypothesis 

plot a histogram 

actual mean percent vote difference is 1.16% 

right : 23% of the simulated data had atleast 1.16% difference or greater 

point-23 is p value 

THe probability of obtaining a value of your test statistic that is atleast as extreme as what was observed , under the assumption the null hypothesis is true probability



def draw_perm_reps(data_1, data_2, func, size=1):
    """Generate multiple permutation replicates."""

    # Initialize array of replicates: perm_replicates
    perm_replicates = np.empty(size)

    for i in range(size):
        # Generate permutation sample
        perm_sample_1, perm_sample_2 = permutation_sample(data_1,data_2)

        # Compute the test statistic
        perm_replicates[i] = func(perm_sample_1,perm_sample_2)

    return perm_replicates


Kleinteich and Gorb (Sci. Rep., 4, 5225, 2014) performed an interesting experiment with South American horned frogs.
 They held a plate connected to a force transducer, along with a bait fly, in front of them. 
 They then measured the impact force and adhesive force of the frogs tongue when it struck the target.
Frog A is an adult and Frog B is a juvenile. The researchers measured the impact force of 20 strikes for each frog. 
In the next exercise, we will test the hypothesis that the two frogs have the same distribution of impact forces. 
But, remember, it is important to do EDA first! Lets make a bee swarm plot for the data. 
They are stored in a Pandas data frame, df, where column ID is the identity of the frog and column impact_force is the impact force in Newtons (N).

# Make bee swarm plot
_ = sns.swarmplot(x='ID',y='impact_force',data = df)

# Label axes
_ = plt.xlabel('frog')
_ = plt.ylabel('impact force (N)')

# Show the plot
plt.show()


Eyeballing it, it does not look like they come from the same distribution. 
Frog A, the adult, has three or four very hard strikes, and Frog B, the juvenile, has a couple weak ones. 
However, it is possible that with only 20 samples it might be too difficult to tell if they have difference distributions, 
so we should proceed with the hypothesis test.


Permutation test on frog data 

The average strike force of Frog A was 0.71 Newtons (N), and that of Frog B was 0.42 N for a difference of 0.29 N. 
It is possible the frogs strike with the same force and this observed difference was by chance. 
You will compute the probability of getting at least a 0.29 N difference in mean strike force under the hypothesis that
 the distributions of strike forces for the two frogs are identical. We use a permutation test with a test statistic of the 
 difference of means to test this hypothesis.

For your convenience, the data has been stored in the arrays force_a and force_b.

def diff_of_means(data_1, data_2):
    """Difference in means of two arrays."""

    # The difference of means of data_1, data_2: diff
    diff = np.mean(data_1) - np.mean(data_2)

    return diff

# Compute difference of mean impact force from experiment: empirical_diff_means
empirical_diff_means = diff_of_means(force_a,force_b)

# Draw 10,000 permutation replicates: perm_replicates
perm_replicates = draw_perm_reps(force_a, force_b,
                                 diff_of_means, size=10000)

# Compute p-value: p
p = np.sum(perm_replicates >= empirical_diff_means) / len(perm_replicates)

# Print the result
print('p-value =', p)

p = 0.0063

The p-value tells you that there is about a 0.6% chance that you would get the difference of means observed in the experiment 
if frogs were exactly the same. A p-value below 0.01 is typically said to be "statistically significant,"
 but: warning! warning! warning! You have computed a p-value; it is a number. I encourage you not to distill it to a yes-or-no phrase.
 p = 0.006 and p = 0.000000006 are both said to be "statistically significant," but they are definitely not the same!


 Bootstrap hypothesis tests 

 pipeline for hypothesis testing 

 clearly state null hypothesis
 Define your test statistic 
 Generate many sets of simulated data assuming null hypothesis is true 
 compute the test statistic for each simulated data 
 The p-value is the fraction of your simulated data sets for which the test statistic is atleast as extreme as for the real data 


 Michelson and Newcomb: speed of light pioneers 

 299,852
 299,860

data points are not available in Newcomb

 Null hypothesis :

 The true mean speed of light in michelsons experiments was actually New combs reported value

 shifting the michelson data to match the mean of newcomb data 

 newcomb_value = 299860
 michelson_shifted = michelson_speed_of_light - np.mean(michelson_speed_light) + newcomb_value

 see the edf of michelson data with same mean as newcomb and michelson data 

 x axis -> speed of light 


 we cannot use permutation replicates as the data is not available for new comb 

 we can use bootstrapping on the shifted data to simulate data acquisition under the null hypothesis

 The test statistic: mean of bootstrap sample - mean of newcomb value 


 def diff_from_newcomb(data,newcomb_value=299860):
    return np.mean(data) - newcomb_value

diff_obs = diff_from_newcomb(michelson_speed_of_light)

-7.59

bs_replicates = draw_bs_reps(michelson_shifted,diff_from_newcomb,10000)

p_value = np.sum(bs_replicates <= diff_obs)/10000

0.1603


one sample test:
compare one set of data to a single number 

tow sample test:
compare two sets of data 

Explicitly simulating a null hypothesis like this , where we have to shift the mean can be tricky 

A one sample bootstrap hypothesis test
Another juvenile frog was studied, Frog C, and you want to see if Frog B and Frog C have similar impact forces. Unfortunately, you do not have Frog C's impact forces available, but you know they have a mean of 0.55 N. Because you don't have the original data, you cannot do a permutation test, and you cannot assess the hypothesis that the forces from Frog B and Frog C come from the same distribution. You will therefore test another, less restrictive hypothesis: The mean strike force of Frog B is equal to that of Frog C.

To set up the bootstrap hypothesis test, you will take the mean as our test statistic. 
Remember, your goal is to calculate the probability of getting a mean impact force less than or equal to what was observed for Frog B 
if the hypothesis that the true mean of Frog Bs impact forces is equal to that of Frog C is true.
 You first translate all of the data of Frog B such that the mean is 0.55 N.
  This involves adding the mean force of Frog C and subtracting the mean force of Frog B from each measurement of Frog B. 
This leaves other properties of Frog B's distribution, such as the variance, unchanged.'

# Make an array of translated impact forces: translated_force_b
translated_force_b = force_b + 0.55 - np.mean(force_b)

# Take bootstrap replicates of Frog B's translated impact forces: bs_replicates
bs_replicates = draw_bs_reps(translated_force_b, np.mean, 10000)

# Compute fraction of replicates that are less than the observed Frog B force: p
p = np.sum(bs_replicates <= np.mean(force_b)) / 10000

# Print the p-value
print('p = ', p)


p= 0.0046

Great work! The low p-value suggests that the null hypothesis that Frog B and Frog C have the same mean impact force is false.

tow sample bootstrap hypothesis test for difference of means :

We now want to test the hypothesis that Frog A and Frog B have the same mean impact force, but not necessarily the same distribution, which is also impossible with a permutation test.
To do the two-sample bootstrap test, we shift both arrays to have the same mean, since we are simulating the hypothesis that their means are, in fact, equal. We then draw bootstrap samples out of the shifted arrays and compute the difference in means. This constitutes a bootstrap replicate, and we generate many of them. The p-value is the fraction of replicates with a difference in means greater than or equal to what was observed.
The objects forces_concat and empirical_diff_means are already in your namespace.

# Compute mean of all forces: mean_force
mean_force = np.mean(forces_concat)

# Generate shifted arrays
force_a_shifted = force_a - np.mean(force_a) + mean_force
force_b_shifted = force_b - np.mean(force_b) + mean_force 

# Compute 10,000 bootstrap replicates from shifted arrays
bs_replicates_a = draw_bs_reps(force_a_shifted, np.mean, 10000)
bs_replicates_b = draw_bs_reps(force_b_shifted, np.mean, 10000)

# Get replicates of difference of means: bs_replicates
bs_replicates = bs_replicates_a - bs_replicates_b

# Compute and print p-value: p
p = np.sum(bs_replicates>= empirical_diff_means) / len(bs_replicates)
print('p-value =', p)




