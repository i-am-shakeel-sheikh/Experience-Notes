we will introduce respone objects in scrapy which behave like selectors but give us extra tools to mobilize to multiple websites

we are moving towards creating spiders , programs that crawl the web and scrape the data in a way we specify 

selector objects -> response objects

Response also keeps track of the url where the HTML code was loaded from
response helps us move from one site to another, so that we can crawl the web while scraping

response.url
response.follow(next_url)


css_locator = 'a.course-block__link'
response_as = response.css(css_locator)
sel_as = sel.css(css_locator)
# Examine similarity
nr = len( response_as )
ns = len( sel_as )
for i in range( min(nr, ns, 2) ):
  print( "Element %d from response: %s" % (i+1, response_as[i]) )
  print( "Element %d from sel: %s" % (i+1, sel_as[i]) )
  print( "" )

---------------------
# Select all desired div elements
divs = response.css('div.course-block')
first_div = divs[0]
h4_text = first_div.css('h4::text').extract_first()
print( "The text from the h4 element is:", h4_text )
----------------------

Scraping for reals

https://www.datacamp.com/courses/all

we will scrape this and result is list of strings that point to different courses
185 courses listed in the directory

course_divs = response.css('div.course-block')
print(len(course_divs))
185

first_div = course_divs[0]
children = first_div.xpath('./*')
print(len(children))
3

first_child = children[0] -> hyper link to the course website
print(first_child.extract())
<a class=... />

second_child = children[1]
print(second_child.extract())
<div class=... />

third_child = children[3]
<span class=.../>


links = response.css('div.course-block > a::attr(href)').extract()

for l in links:
	print(l)

--------------

# Create a SelectorList of the course titles
crs_title_els = response.css('h4::text')
# Extract the course titles 
crs_titles = crs_title_els.extract()

for el in crs_titles:
  print( ">>", el )
------------------

how_many_kids = len( mystery.xpath( './*' ) )

-------------------

classy spider

crawl the web through multiple pages following the links we choose and scrape those pages automatically according to the procedures we programmed

import scrapy
from scrapy.crawler import CrawlerProcess

class SpiderClassName(scrapy.Spider):
	name = "spider_name"
	# the code for ur spider
	
#initiate a CrawlerProcess
process = CrawlerProcess()

#tell the process which spider to use
process.crawl(Yourspider)

process.start()

-------------------------

Inheriting the spider

# Import scrapy library
import scrapy
# Create the spider class
class YourSpider(scrapy.Spider):
  name = "your_spider"
  # start_requests method
  def start_requests(self):
    pass
  # parse method
  def parse(self, response):
    pass
# Inspect Your Class
inspect_class(YourSpider)

-----------------

start Requests

class DCSpider( scrapy.Spider):
	name = "dc_spider"

	def start_requests( self ):
		urls = ['https://www.datacamp.com/courses/all']
		for url in urls:
			yield scrapy.Request(url = url, callback = self.parse)

	def parse(self, response):
		html_file = 'DC_courses.html'
		with open(html_file , 'wb') as fout:
			fout.write(response.body)

scrapy.Request here will fill in a response variable for us
The url argument tells us which site to scrape
callback tells where to send the response variable for processing

----------------------

# Import scrapy library
import scrapy

# Create the spider class
class YourSpider( scrapy.Spider ):
  name = "your_spider"
  # start_requests method
  def start_requests( self ):
    self.print_msg("Hello World!")
  # parse method
  def parse( self, response ):
    pass
  # print_msg method
  def print_msg( self, msg ):
    print( "Calling start_requests in YourSpider prints out:", msg )
  
# Inspect Your Class
inspect_class( YourSpider )
-----------------------------

parse and crawl

class DCSpider( scrapy.Spider ):
	name = "dc_spider"

	def parse(self,response):
		links = response.css('div.course-block > a::attr(href)').extract()
		filepath = 'DC_links.csv'
		with open(filepath, 'w') as f :
			f.writelines([link + '/n' for link in links])
		for link in links:
			yield response.follow(url = link, callback = self.parse2)

	def parse2(self, response):

follow method works similar to scrapy Request 

-----------------------

# Import the scrapy library
import scrapy

# Create the Spider class
class DCspider( scrapy.Spider ):
  name = 'dcspider'
  # start_requests method
  def start_requests( self ):
    yield scrapy.Request( url = url_short, callback = self.parse )
  # parse method
  def parse( self, response ):
    # Create an extracted list of course author names
    author_names = response.css('p.course-block__author-name::text').extract()
    # Here we will just return the list of Authors
    return author_names
  
# Inspect the spider
inspect_spider( DCspider )

------------------------------

This will be your first chance to play with a spider which will crawl between sites (by first collecting links from one site and following 
	those links to parse new sites )

# Import the scrapy library
import scrapy

# Create the Spider class
class DCdescr( scrapy.Spider ):
  name = 'dcdescr'
  # start_requests method
  def start_requests( self ):
    yield scrapy.Request( url = url_short, callback = self.parse )
  
  # First parse method
  def parse( self, response ):
    links = response.css( 'div.course-block > a::attr(href)' ).extract()
    # Follow each of the extracted links
    for link in links:
      yield response.follow(url = link, callback = self.parse_descr)
      
  # Second parsing method
  def parse_descr( self, response ):
    # Extract course description
    course_descr = response.css( 'p.course__description::text' ).extract_first()
    # For now, just yield the course description
    yield course_descr


# Inspect the spider
inspect_spider( DCdescr )

-----------------------------

cap stone 

we can set up entire scrapy spider from start to finish 
spider which collects all the courses from the datacamp course directory
then follow those links to extract course title and description , chapters
finally it will store the information in dictionary for us to use as we want later

import scrapy
from scrapy.crawler import CrawlerProcess

class DC_chapter_Spider(scrapy.Spider):
	name = "DC_chapter_Spider"

	def start_requests(self):
		url = "https://www.datacamp.com/courses/all"
		yield scrapy.Request(url = url, callback = self.parse_front)

	def parse_front(self, response):
		##code to parse the front courses page
		reponse.css('div.course-block')
		#Direct to the course links
		course_links = response.xpath('./a/@href')
		links_to_follow = course_links.extract()
		for url in links_to_follow:
			yield response.follow(url = url, callback = self.parse_pages)
		
	def parse_pages(self, response):
		#code to parse courses page
		crs_title = response.xpath('//h1[contains(@class,'title')]/text()')
		#extract and clean the course title
		crs_title_ext = crs_title.extract_first().strip()  #remove strange character returns of html
		chp_title = response.xpath('//h4[containts(@class,'chapter__title')]/text()')
		chp_title = response.css('h4.chapter__title::text')
		chp_titles_ext = [title.strip() for title in chp_title.extract()]
		#fill in the dc_dict here
		dc_dict[crs_title_ext] = chp_titles_ext

dc_dict = dict()

process = CrawlerProcess()
process.crawl(DC_chapter_Spider)
process.start()


cours_titles = keys
list of chapter titles as items


1. direct to all course-block div elements

course tile is present in h1 tag whose tag containts title clas
chapter is present in h4 tag whose tag contains clas chapter__title

{
	"country": ["india", "pakisthan"]
}

valid json

# Import scrapy
import scrapy

# Import the CrawlerProcess
from scrapy.crawler import CrawlerProcess

# Create the Spider class
class YourSpider(scrapy.Spider):
  name = 'yourspider'
  # start_requests method
  def start_requests( self ):
    yield scrapy.Request(url = url_short, callback=self.parse)
      
  def parse(self, response):
    # My version of the parser you wrote in the previous part
    crs_titles = response.xpath('//h4[contains(@class,"block__title")]/text()').extract()
    crs_descrs = response.xpath('//p[contains(@class,"block__description")]/text()').extract()
    for crs_title, crs_descr in zip( crs_titles, crs_descrs ):
      dc_dict[crs_title] = crs_descr
    
# Initialize the dictionary **outside** of the Spider class
dc_dict = dict()

# Run the Spider
process = CrawlerProcess()
process.crawl(YourSpider)
process.start()

# Print a preview of courses
previewCourses(dc_dict)
------------------------

How to process the data that is already extracted 
supervised , unsupervised, clustering , deep learning
rarely deal with the data on how to extract 

Scraping skills

identified a website or a collection of websites with the information we want to process

Objective: scrape a website computationally

selector and response objects

tell the scrapy which elements to select 

xpath or css locator notation

structure of html 
---------------------
