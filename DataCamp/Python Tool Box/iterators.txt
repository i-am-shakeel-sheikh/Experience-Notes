Iterators in pythonland

Iterators, list comprehensions, generators 

for i in range(4):
	print(i) 

The reason we can loop over such objects is they are special objects called Iterables 

Iterable:
	lists, strings, dictionaries, file connections 

Actual defination of iterable is an object that has an associated iter method 
iter method is applied to iterable, iterator object is created 

under the hood for loop is doing this 
takes an iterable , creates an associated iterator object and iterates over it 

An iterator is an object that has an associated next() method that produces consecutive values 


word = 'Da'
it = iter(word)
next(it)
next(it)

it = iter(word)
print(*it)

file = open('file.txt')
it = iter(file)
print(next(it))



flash = ['jay garrick', 'barry allen', 'wally west', 'bart allen']
for person in flash:
    print(person)
superspeed = iter(flash)
print(next(superspeed))
print(next(superspeed))
print(next(superspeed))
print(next(superspeed))

------------

small_value = iter(range(3))
print(next(small_value))
print(next(small_value))
print(next(small_value))
for num in range(3):
    print(num)
googol = iter(range(10**100))
print(next(googol))
print(next(googol))
print(next(googol))
print(next(googol))
print(next(googol))

------------------

values = range(10,21)
print(values)
values_list = list(values)
print(values_list)
values_sum = sum(range(10,21))
print(values_sum)

--------------

playing with Iterators

enumerate() counter to any iterable 
zip() stich together an arbitrary number of iterables 

avengers = ['hawkeye','ironman','thor','quicksliver']
e = enumerate(avengers)
print(type(e))
<'class'enumerate>

e_list = list(e)

contains a list of tuples

[(0,'hawkeye'),(1,'iron man'),(2, 'thor'),(3,'quicksliver')]


for index,value in enumerate(avengers):
	print(index,value)


zip() function 
accepts an arbitrary number of iterables returns a iterator of tuples 

avengers = ['hawkeye','ironman','thor','quicksliver']
names = ['barton','stark','odinson','maximoff']
z = zip(avengers,names)
print(type(z))
z_list = list(z)
print(z_list)

[('hawkeye','barton'),('ironman','stark')]


Excercises 


# Create a list of strings: mutants
mutants = ['charles xavier', 
            'bobby drake', 
            'kurt wagner', 
            'max eisenhardt', 
            'kitty pryde']

mutant_list = list(enumerate(mutants))
print(mutant_list)
for index1,value1 in enumerate(mutants):
    print(index1, value1)
for index2,value2 in enumerate(mutants,start=1):
    print(index2, value2)



mutant_data = list(zip(mutants,aliases,powers))
print(mutant_data)
mutant_zip = zip(mutants,aliases,powers)
print(mutant_zip)
for value1,value2,value3 in mutant_zip:
    print(value1, value2, value3)



z1 = zip(mutants,powers)
print(*z1)
z1 = zip(mutants,powers)
result1, result2 = zip(*z1)

# Check if unpacked tuples are equivalent to original tuples
print(result1 == mutants)
print(result2 == powers)

-----------------------

using iterators to load large files into memory

pulling data from a file, database or API 
one solution 
load data in chunks 


csv file: column x : compute all the numbers present in that column
file is too large to store in memory

import pandas as pd 

result = []
 
for chunk in pd.read_csv('data.csv',chunksize=1000):
	result.append(sum(chunk['x']))

total = sum(result)


--------------------------

# Initialize an empty dictionary: counts_dict
counts_dict={}

# Iterate over the file chunk by chunk
for chunk in pd.read_csv('tweets.csv',chunksize=10):

    # Iterate over the column in DataFrame
    for entry in chunk['lang']:
        if entry in counts_dict.keys():
            counts_dict[entry] += 1
        else:
            counts_dict[entry] = 1

# Print the populated dictionary
print(counts_dict)


-----
# Define count_entries()
def count_entries(csv_file,c_size,colname):
    counts_dict = {}
    for chunk in pd.read_csv(csv_file,chunksize=c_size):
        for entry in chunk[colname]:
            if entry in counts_dict.keys():
                counts_dict[entry] += 1
            else:
                counts_dict[entry] = 1

    # Return counts_dict
    return counts_dict

# Call count_entries(): result_counts
result_counts = count_entries('tweets.csv',10,'lang')
print(result_counts)



Another way to read data too large to store in memory in chunks is to read the file in as DataFrames of a certain length, say, 100. 
For example, with the pandas package (imported as pd), you can do pd.read_csv(filename, chunksize=100). This creates an iterable reader object, 
which means that you can use next() on it.








